# -*- coding: utf-8 -*-
"""AirBnb price proposition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uTu4gqxiTuvWn6Ear24kRlarfzNu86tq
"""

#Libraries
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
dl = pd.read_csv('/content/drive/My Drive/Portfolio/Airbnb data/listings.csv', encoding='ISO-8859-1')

pd.set_option('display.max_columns', None)
dl.head(5)

"""#CLEANING"""

#Overview valeurs manquantes

pd.set_option('display.max_rows', None)
print(dl.isna().sum())
pd.reset_option('display.max_rows', None)

unique_neighborhoods = dl['neighbourhood_cleansed'].unique()
print(unique_neighborhoods)

dl.shape

dl.columns

#Garder ce qui nous interesse et peut etre utilisé

dlc = dl[['id',
#'listing_url',
#'scrape_id', 'last_scraped', 'source', 'name',
#'description', 'neighborhood_overview', 'picture_url',
    'host_id',
#'host_url', 'host_name',
    'host_since',
#'host_location',
#'host_about',
#'host_response_time', 'host_response_rate', 'host_acceptance_rate',
       'host_is_superhost',
#'host_thumbnail_url', 'host_picture_url',
#'host_neighbourhood', 'host_listings_count',
#'host_total_listings_count',
#'host_verifications',
#'host_has_profile_pic',
    'host_identity_verified',
#'neighbourhood',
       'neighbourhood_cleansed',
#'neighbourhood_group_cleansed', 'latitude','longitude',
          'property_type', 'room_type', 'accommodates',
#'bathrooms',
#'bathrooms_text',
    'bedrooms',
#'beds',
#'amenities',
    'price',

#'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',
#  'maximum_minimum_nights', 'minimum_maximum_nights',
# 'maximum_maximum_nights', 'minimum_nights_avg_ntm',
#'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',
#'availability_30', 'availability_60', 'availability_90',
      'availability_365',
#'calendar_last_scraped',
    'number_of_reviews',
#'number_of_reviews_ltm',
#'number_of_reviews_l30d', 'first_review',
#'last_review',
    'review_scores_rating',
#'review_scores_accuracy',
#'review_scores_cleanliness', 'review_scores_checkin',
#'review_scores_communication', 'review_scores_location',
       'review_scores_value',
#'license',
          'instant_bookable',
#'calculated_host_listings_count',
#'calculated_host_listings_count_entire_homes',
#'calculated_host_listings_count_private_rooms',
#'calculated_host_listings_count_shared_rooms', 'reviews_per_month'
          ]].copy()

# Etudier les valeurs manquantes

pd.set_option('display.max_rows', None)
print(dlc.isna().sum())
pd.reset_option('display.max_rows', None)

## Créer dlsp qu'on utilisera pour tester à l'avenir

dlsp = dlc[dlc['price'].isna()].copy()
dlsp.head(5)

## Je commence a cleaner, je me prends pas la tête je fais simple je supprime si na

dlc = dlc[dlc['price'].notna()].copy()
pd.set_option('display.max_rows', None)
print(dlc.isna().sum())
pd.reset_option('display.max_rows', None)

dlc.shape

dlc = dlc[dlc['review_scores_value' ].notna()].copy()
pd.set_option('display.max_rows', None)
print(dlc.isna().sum())
pd.reset_option('display.max_rows', None)

dlc = dlc[dlc['bedrooms' ].notna()].copy()
pd.set_option('display.max_rows', None)
print(dlc.isna().sum())
pd.reset_option('display.max_rows', None)

dlc.shape

# si pas de super host je remplace par f

unique_host_is_superhost = dlc['host_is_superhost'].unique()
print(unique_host_is_superhost)

#je verifie
dlc['host_is_superhost'] = dlc['host_is_superhost'].fillna('f')

unique_host_is_superhost = dlc['host_is_superhost'].unique()
print(unique_host_is_superhost)

# je regarde les types pour créer mon modele par la suite


dlc.dtypes

"""#Data prep"""

dlc['price'] = dlc['price'].replace('[\$,]', '', regex=True).astype(float)
dlc['price_per_person'] = dlc['price'] / dlc['accommodates'].replace(0, np.nan)

dlc['price_per_person'].describe()

dlc['price_per_person']

dlc = dlc[(dlc['price_per_person'] >= 10) & (dlc['price_per_person'] <= 200)]

# Affichage du nombre de lignes après suppression pour confirmation
print(dlc.shape)

dlc['price'].describe()

plt.figure(figsize=(10, 6))
sns.boxplot(x=dlc['price_per_person'], color='lightgreen')
plt.xlabel('Prix')
plt.title('Boxplot des Prix des Annonces AirBnB')
plt.show()

prices_log = np.log1p(dlc['price_per_person'])  # Logarithme des prix

plt.figure(figsize=(10, 6))
plt.hist(prices_log, bins=50, color='coral', edgecolor='black')
plt.xlabel('Log(Prix)')
plt.ylabel('Fréquence')
plt.title('Distribution Logarithmique des Prix des Annonces AirBnB')
plt.show()

plt.figure(figsize=(10, 6))
plt.hist(dlc['price_per_person'], bins=50, color='skyblue', edgecolor='black')
plt.xlabel('Prix')
plt.ylabel('Fréquence')
plt.title('Distribution des Prix des Annonces AirBnB')
plt.show()

"""## pas utilisé"""

# numérique + symbole
Q1 = dlc['price'].quantile(0.25)
Q3 = dlc['price'].quantile(0.75)
IQR = Q3 - Q1

# Définition limites
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# supréssion les valeurs aberrantes
dlc = dlc[(dlc['price'] >= lower_bound) & (dlc['price'] <= upper_bound)]
dlc.shape
dlc['price'].describe()
print("Lower bound:", lower_bound)
print("Upper bound:", upper_bound)

"""#cleaning suite"""

# Convertion booléennes
dlc['host_is_superhost'] = dlc['host_is_superhost'].map({'t': True, 'f': False})
dlc['instant_bookable'] = dlc['instant_bookable'].map({'t': True, 'f': False})

# Convertir 'host_since' en type datetime
dlc['host_since'] = pd.to_datetime(dlc['host_since'])

#Convertion
dlc = pd.get_dummies(dlc, columns=['property_type', 'room_type', 'neighbourhood_cleansed', 'host_identity_verified'], drop_first=True)

# Remplace date by nb of years
dlc['host_days_active'] = (pd.to_datetime("today") - dlc['host_since']).dt.days

dlc = dlc.drop(columns=['host_since'])
print(dlc[['host_days_active']].head())

X = dlc.drop(columns=['price'])
y = dlc['price'].astype(float)

dlc

## Modifier pour dlcl ???

dlcl = dlc.drop(columns=['id', 'host_id', 'availability_365', 'instant_bookable', 'host_days_active' ])

X = dlcl.drop(columns=['price', 'price_per_person'])
y = dlc['price_per_person']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#XGBoost"""

model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, subsample=0.8, random_state=42)

# Entraîner le modèle
model.fit(X_train, y_train)

# 4. Évaluation du modèle
# Prédire sur l'ensemble de test
y_pred = model.predict(X_test)

# Calculer les métriques d'évaluation
rmse = mean_squared_error(y_test, y_pred, squared=False)
r2 = r2_score(y_test, y_pred)

print("RMSE:", rmse)
print("R²:", r2)

# Définir le modèle de base XGBoost
#model = xgb.XGBRegressor(random_state=42)

# Définir la grille des hyperparamètres
#param_grid = {
    #'#n_estimators': [100, 200, 300],
    #'#learning_rate': [0.01, 0.1, 0.2],
    #'#max_depth': [4, 6, 8],
    #'#subsample': [0.8, 1.0],
    #'colsample_bytree': [0.7, 1.0]
#}

# Initialiser GridSearchCV
#grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           #scoring='neg_mean_squared_error', cv=3, verbose=1)

# Entraîner la recherche en grille sur les données
#grid_search.fit(X_train, y_train)

# Meilleure combinaison d'hyperparamètres
#print("Meilleurs paramètres : ", grid_search.best_params_)
#print("Meilleure performance (négative MSE) : ", grid_search.best_score_)

model = xgb.XGBRegressor(n_estimators=300, learning_rate=0.1, max_depth=6, subsample=0.8, random_state=42, colsample_bytree=0.7)

# Entraîner le modèle
model.fit(X_train, y_train)

# 4. Évaluation du modèle
# Prédire sur l'ensemble de test
y_pred = model.predict(X_test)

# Calculer les métriques d'évaluation
rmse = mean_squared_error(y_test, y_pred, squared=False)
r2 = r2_score(y_test, y_pred)

print("RMSE:", rmse)
print("R²:", r2)

"""#Neural Network"""

# Normalisation des données pour que toutes les caractéristiques soient sur la même échelle
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 2. Définition du Réseau de Neurones
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.0005)))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))
model.add(Dropout(0.3))
model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))
model.add(Dropout(0.3))
model.add(Dense(1, activation='linear'))

# Compilation du modèle
model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])

# Early stopping pour éviter le surapprentissage
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# 3. Entraînement du Modèle
history = model.fit(X_train, y_train,
                    validation_split=0.2,  # Utilisation d'une partie des données d'entraînement pour la validation
                    epochs=100,
                    batch_size=32,
                    callbacks=[early_stop],
                    verbose=1)

# 4. Évaluation du Modèle
loss, mae = model.evaluate(X_test, y_test, verbose=1)
print("Mean Absolute Error (MAE) sur le test set:", mae)

# Normalisation des données pour que toutes les caractéristiques soient sur la même échelle
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 2. Définition du Réseau de Neurones
model = Sequential()
model.add(Dense(256, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.0005)))
model.add(Dropout(0.2))
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.0005)))
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.0005)))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.0005)))
model.add(Dropout(0.2))
model.add(Dense(1, activation='linear'))

# Compilation du modèle
model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])

# Early stopping pour éviter le surapprentissage
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# 3. Entraînement du Modèle
history = model.fit(X_train, y_train_log,
                    validation_split=0.2,  # Utilisation d'une partie des données d'entraînement pour la validation
                    epochs=100,
                    batch_size=32,
                    callbacks=[early_stop],
                    verbose=1)

y_pred_log = model.predict(X_test)
y_pred = np.expm1(y_pred_log)
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, y_pred)  # Calcul sur l’échelle d’origine
print("Mean Absolute Error (MAE) sur le test set:", mae)

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.show()

"""#Calculateur"""

new_listing = {
    'room_type': 'Entire home/apt',  # Exemple de type de logement
    'neighbourhood_cleansed': 'Gobelins',  # Exemple de quartier
    'accommodates': 4,
    'bedrooms': 2,
    'bathrooms': 1,
    'beds': 2,
    'host_is_superhost': 'f',  # Booléen pour superhost ou non
    'instant_bookable': 'f',   # Booléen pour réservation instantanée ou non
    'host_days_active': 365    # Exemple d'ancienneté de l'hôte
}

# Convertir les informations en DataFrame
new_listing_df = pd.DataFrame([new_listing])

# Appliquer les mêmes transformations que pour l’entraînement
# 1. Encodage des variables catégorielles
new_listing_df = pd.get_dummies(new_listing_df, drop_first=True)

column_names = X.columns

# 2. Ajouter des colonnes manquantes et réordonner `new_listing_df` pour correspondre à `X_train`
for col in column_names:
    if col not in new_listing_df.columns:
        new_listing_df[col] = 0
new_listing_df = new_listing_df[column_names]  # Réordonne pour correspondre à `X_train`

# 3. Normaliser les données avec le scaler
new_listing_df = scaler.transform(new_listing_df)

# 4. Prédire le prix
predicted_price = model.predict(new_listing_df)

print(f"Le prix suggéré pour cette annonce est : ${predicted_price[0][0]:.2f} par nuit.")